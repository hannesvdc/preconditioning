import autograd.numpy as np
import autograd.numpy.linalg as lg
import autograd.numpy.random as rd
import matplotlib as mpl
import matplotlib.pyplot as plt
import scipy.optimize as opt
import scipy.optimize.nonlin as nl

from autograd import jacobian

import api.NewtonKrylovRecursiveNet as recnet
import api.Scheduler as sch
import api.algorithms.Adam as adam
import api.algorithms.BFGS as bfgs

# General setup routine shared by all training routines
def setupRecNet(outer_iterations=3, inner_iterations=4, baseweight=4.0):
    # Define the Chandresakhar H-function
    c = 0.875
    m = 10
    I = np.ones(m)
    mu = (np.arange(1, m+1, 1) - 0.5) / m
    def computeAc():
        Ac = np.zeros((10,10))
        for i in range(10):
            for j in range(10):
                Ac[i,j] = mu[i] / (mu[i] + mu[j])
        return 0.5 * c/m * Ac
    Ac = computeAc()
    H = lambda x: x + np.divide(I, I + np.dot(Ac, x))

    # Sample data - the inittial conditions x_0,i, i = data index
    N_data = 1000
    rng = rd.RandomState()
    x0_data = rng.normal(1.0, np.sqrt(0.2), size=(m, N_data))

    # Setup classes for training
    net = recnet.NewtonKrylovSuperStructure(H, x0_data, outer_iterations, inner_iterations, baseweight=baseweight)
    f = lambda w: net.loss(w)
    df = jacobian(f)

    return net, f, df, H

def sampleWeights(net, threshold=1.e6):
    rng = rd.RandomState()
    inner_iterations = net.inner_iterations
    n_weights = (inner_iterations * (inner_iterations + 1) ) // 2

    print('Selecting Proper Random Initial Condition')
    while True:
        weights = rng.normal(size=n_weights)
        if net.loss(weights) < threshold:
            return weights

# Only used to train Newton-Krylov network with 10 inner iterations
def trainNKNetAdam():
    net, f, df, H = setupRecNet(outer_iterations=3, inner_iterations=10)
    weights = sampleWeights(net, threshold=1.e3)
    print('Initial Loss', f(weights))
    print('Initial Loss Derivative', lg.norm(df(weights)))

    # Setup the optimizer
    scheduler = sch.PiecewiseConstantScheduler({0: 1.e-2, 2: 1.e-3, 5000: 1.e-4, 10000: 1.e-5})
    optimizer = adam.AdamOptimizer(f, df, scheduler=scheduler)
    print('Initial weights', weights)

    # Do the training
    epochs = 15000
    try:
        weights = optimizer.optimize(weights, n_epochs=epochs)
    except KeyboardInterrupt: # If Training has converged well enough with Adam, the user can stop manually
        print('Aborting Training. Plotting Convergence')
    print('Done Training at', len(optimizer.losses), 'epochs. Weights =', weights)
    losses = np.array(optimizer.losses)
    grad_norms = np.array(optimizer.gradient_norms)

    # Post-processing
    x_axis = np.arange(len(losses))
    plt.grid(linestyle = '--', linewidth = 0.5)
    plt.semilogy(x_axis, losses, label='Training Loss')
    plt.semilogy(x_axis, grad_norms, label='Loss Gradient')
    plt.xlabel('Epoch')
    plt.title('Adam')
    plt.legend()
    plt.show()


# Used to train Newton-Krylov network with 4 inner iterations and fine-tune network with 10 inner iterations
def trainNKNetBFGS():
    inner_iterations = 10
    net, f, df, H = setupRecNet(outer_iterations=3, inner_iterations=inner_iterations)
    if inner_iterations == 4:
        weights = sampleWeights(net, threshold=1.e3)
    else: # fine-tuning, these are weights generated by the Adam optimizer.
        weights = np.array([ 0.47812847, -0.45483572, -0.11603074, -0.63924282, -0.37089748, -1.55015063,
                        0.89811809 ,-0.1144241  , 0.94715849 ,-0.09474457 ,-0.63707962 ,-1.2807364,
                        2.86000934 ,-0.52270491 , 1.09163349 , 0.56606191 , 0.80883884 ,-1.62246751,
                        2.48184562 , 0.73501998 , 1.01818702 ,-1.29777517 ,-0.30559098 , 1.0022361,
                        -0.10960734, -0.27518966,  1.78046596, -1.95616288, -0.79266375, -0.69384947,
                        -1.19333132,  0.69332808,  0.61942116, -1.18160397,  0.02321921,  0.50432789,
                        0.11550956 ,-1.35941475 , 0.33480007 , 0.48562682 , 1.35950217 ,-0.4613935,
                        0.19766032 , 0.19022258 , 1.31284791 , 0.55588222 ,-1.44545237 , 0.52962473,
                        0.774306   ,-0.1651254  , 0.24734028 ,-0.55782324 , 0.48371373 ,-0.90673562,
                        -0.08288336])
    print('Initial Loss', f(weights))
    print('Initial Loss Derivative', lg.norm(df(weights)))

    losses = []
    grad_norms = []
    epoch_counter = [0]
    def callback(x):
        print('\nEpoch #', epoch_counter[0])
        l = f(x)
        g = lg.norm(df(x))
        losses.append(l)
        grad_norms.append(g)
        epoch_counter[0] += 1
        print('Loss =', l)
        print('Gradient Norm =', g)
        print('Weights', x)

    epochs = 5000
    method = 'BFGS'
    result = opt.minimize(f, weights, jac=df, method=method,
                                              options={'maxiter': epochs, 'gtol': 1.e-100}, 
                                              callback=callback)
    weights = result.x
    print('Minimzed Loss', f(weights), df(weights))
    print('Minimization Result', result)

    # Post-processing
    x_axis = np.arange(len(losses))
    plt.grid(linestyle = '--', linewidth = 0.5)
    plt.semilogy(x_axis, losses, label='Training Loss')
    plt.semilogy(x_axis, grad_norms, label='Loss Gradient')
    plt.xlabel('Epoch')
    plt.title(method)
    plt.legend()
    plt.show()

def testNKNet():
    # Setup the network and load the weights. All training done using BFGS routine above.
    net_4, _, _, H = setupRecNet(outer_iterations=3, inner_iterations=4)
    #weights_4 = np.array([  -1.37418947, -0.14445959,   2.51583727, -17.92557257, -112.11663472,
    #                           44.1241637, -4.88042119,  -50.64821349, 18.05257709, -0.54452882])
    weights_4 = np.array([ -1.13845701,  -21.48290808, 151.35155532, -14.0296891,   82.79574916,
                            0.47406062,  -6.7392919,   45.33721111,  -1.37527638,   1.01790898])
    net_10, _, _, _ = setupRecNet(outer_iterations=3, inner_iterations=10)
    weights_10 = np.array([ 0.97085063,  0.1844569,  -1.62411767, -0.92552457, -1.9057041,   1.67394181,
                            0.39443869, -0.97038114,  3.73034923,  1.93468091, -0.67227227, -0.56013644,
                            3.90733964, -2.47176934,  3.93982011, -1.16981573, -1.23902367, -2.51009642,
                            1.39670533, -3.12474909,  2.93611983, -1.58227896,  0.2404482,   0.18943513,
                           -1.3525295,  -0.19298655,  2.60903272, -2.61685754, -2.23065101, -1.89475015,
                           -2.59957032,  0.56786079, -0.95626096, -3.04156986,  1.95988118,  0.80658487,
                            1.05727932, -0.88565973,  1.17470022,  0.7494027,   2.77010291, -0.32988378,
                           -0.6125731,  -0.89360145,  2.26414461, -1.79091731, -4.23418447, -2.17373994,
                           -1.84413661, -1.90335828, -0.25621881,  0.75907866,  0.48094588, -0.07303365,
                           -0.43058224])

    # Generate test data. Same distribution as training data. Test actual training data next
    m = 10
    N_data = 1000
    rng = rd.RandomState()
    x0_data = rng.normal(1.0, np.sqrt(0.2), size=(m, N_data))

    # Run each rhs through the neural network
    n_outer_iterations = 10 # Does not need be the same as the number the network was trained on.
    errors_4     = np.zeros((N_data, n_outer_iterations+1))
    errors_10    = np.zeros((N_data, n_outer_iterations+1))
    nk_errors_4  = np.zeros((N_data, n_outer_iterations+1))
    nk_errors_10 = np.zeros((N_data, n_outer_iterations+1))
    for n in range(N_data):
        x0 = x0_data[:,n]
        samples_4  = net_4.forward(x0, weights_4, n_outer_iterations)
        samples_10 = net_10.forward(x0, weights_10, n_outer_iterations)

        for k in range(len(samples_4)):
            err = lg.norm(H(samples_4[k]))
            errors_4[n,k] = err

        for k in range(len(samples_10)):
            err = lg.norm(H(samples_10[k]))
            errors_10[n,k] = err

        for k in range(n_outer_iterations+1):
            try:
                x_out = opt.newton_krylov(H,  x0, rdiff=1.e-8, iter=k, maxiter=k, method='gmres', inner_maxiter=1, outer_k=0, line_search=None)
            except nl.NoConvergence as e:
                x_out = e.args[0]
            nk_errors_4[n,k] = lg.norm(H(x_out))

        for k in range(n_outer_iterations+1):
            try:
                x_out = opt.newton_krylov(H, x0, rdiff=1.e-8, iter=k, maxiter=k, method='gmres', inner_maxiter=1, outer_k=0, line_search=None)
            except nl.NoConvergence as e:
                x_out = e.args[0]
            nk_errors_10[n,k] = lg.norm(H(x_out))

    # Average the errors
    avg_errors_4  = np.average(errors_4, axis=0)
    avg_errors_10 = np.average(errors_10, axis=0)
    avg_nk_errors_4  = np.average(nk_errors_4, axis=0)
    avg_nk_errors_10 = np.average(nk_errors_10, axis=0)
    print(avg_errors_10)

    # Plot the errors
    fig, ax = plt.subplots()  
    k_axis = np.linspace(0, n_outer_iterations, n_outer_iterations+1)
    rect = mpl.patches.Rectangle((net_4.outer_iterations+0.5, 1.e-8), 7.5, 70, color='gray', alpha=0.2)
    ax.add_patch(rect)
    plt.semilogy(k_axis, avg_errors_4, label=r'Newton-Krylov Network with $4$ Inner Iterations', linestyle='--', marker='d')
    plt.semilogy(k_axis, avg_errors_10, label=r'Newton-Krylov Network with $10$ Inner Iterations', linestyle='--', marker='d')
    plt.semilogy(k_axis, avg_nk_errors_4, label=r'Scipy with $4$ Krylov Vectors', linestyle='--', marker='d')
    plt.semilogy(k_axis, avg_nk_errors_10, label=r'Scipy with $10$ Krylov Vectors', linestyle='--', marker='d')
    plt.xticks(np.linspace(0, n_outer_iterations, n_outer_iterations+1))
    plt.xlabel(r'# Outer Iterations $k$')
    plt.ylabel(r'$|H(x_k)|$')
    plt.xlim((-0.5,n_outer_iterations + 0.5))
    plt.ylim((0.1*min(np.min(avg_nk_errors_4), np.min(avg_errors_4), np.min(avg_nk_errors_10), np.min(avg_errors_10)),70))
    plt.title(r'Function Value $|H(x_k)|$')
    plt.legend()
    plt.show()

if __name__ == '__main__':
    #trainNKNetAdam()
    #trainNKNetBFGS()
    testNKNet()